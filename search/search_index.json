{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Machine Learning Process","text":""},{"location":"#introduction","title":"Introduction","text":"<p>This learning resource demonstrates a Machine Learning (ML) system for classification of Sentinel-2 images into 10 different classes using cloud-native technologies. The system leverages MLFLOW to track the training process and selects the best candidate trained model from MLFLOW server. </p> <p>There are two workflows developed: one for training a deep learning model classifier on EuroSAT dataset, and one for running a prediction on a real world Sentinel-2 data.  The automation is achieved using Kubernetes-native tools, making the setup scalable, modular, and suitable for Earth Observation and geospatial applications.</p>"},{"location":"#key-components","title":"Key Components","text":"<p>This setup integrates the following technologies and concepts:</p>"},{"location":"#mlflow","title":"MLFLOW","text":"<ul> <li>Manages end-to-end ML workflows, from development to production</li> <li>End-to-end MLOps solution for traditional ML, including integrations with traditional ML models, and Deep learning one</li> <li>Simple, low-code performance tracking with autologging</li> <li>State-of-the-art UI for model analysis and comparison</li> </ul>"},{"location":"#stac-duckdb","title":"STAC &amp; DuckDB","text":"<ul> <li>Acts as the primary data source by providing geospatial data in a standardized format. The STAC collection includes references to the EUROSAT benchmark dataset, which can be queried using DuckDB.</li> </ul>"},{"location":"#high-level-architecture","title":"High-Level Architecture","text":"<p>The system is designed to handle the following flow:</p> <ol> <li> <p>Training pipeline: A CNN model trained on EuroSAT dataset which already exist on a dedicated STAC endpoint. The MLFLOW tracks the whole process to monitor the life cycle of training.</p> </li> <li> <p>Inference: Run the inference pipeline to perform tile-based classification on Sentinel-2 L1C products.</p> </li> <li> <p>Workflow Execution: Both training and inference pipeline will be executed using the CWL-based algorithm.</p> </li> </ol>"},{"location":"#why-use-this-setup","title":"Why Use This Setup?","text":"<p>This setup demonstrates the power of combining machine learning paradigms with container-native workflows to enable scalable geospatial analysis.</p> <p>It is particularly suited for Earth observation and scientific workflows because:</p> <ul> <li>Scalability: Kubernetes ensures workflows can handle varying loads effectively.</li> <li>Modularity: Components can be easily reused or replaced for other applications.</li> <li>Automation: Events trigger workflows without manual intervention, enabling real-time processing.</li> </ul> <p>Through this resource, you'll learn to implement a cloud-native pipeline for tile-based classification, which can be extended to other geospatial or scientific applications.</p>"},{"location":"Describe-MLmodel/","title":"Describe a trained machine learning model","text":"<p>This Notebook leverages the capabilities of STAC to provide a comprehensive and standardized description of a trained ML model. This is done with a STAC Item file that encapsulates the relevant metadata (e.g. model name and version, description of the model architecture and training process, specifications of input and output data formats, etc.). </p> <p>This Notebook can be used for the following requirements:</p> <ul> <li>Import Libraries (e.g. <code>pystac</code>, <code>boto3</code>)</li> <li>Option to either create a STAC Item with <code>pystac</code>, or to upload an existing STAC Item into the Notebook. The STAC Item will contain all related ML model specific properties, related STAC extensions and hyperparameter.</li> <li>Create interlinked STAC Item, Catalog and Collection, and the STAC folder structure </li> </ul> <p>Objective: By the end of this Notebook, the user will have published a STAC Item, Collection and Catalog into the STAC endpoint, and tested its search functionalities via query parameters. </p> <p>Table of Content:</p> <p>1) Import Libraries 2) Create STAC Item, Catalog and Collection</p>"},{"location":"Describe-MLmodel/#1-import-libraries","title":"1) Import Libraries","text":"<pre><code>from datetime import datetime\nimport os\nimport json\nimport requests\nfrom pathlib import Path\nimport concurrent.futures\nfrom concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\nfrom tqdm import tqdm\n\nimport pystac\nfrom pystac import read_file\nfrom pystac.extensions.version import ItemVersionExtension\nfrom pystac.extensions.eo import EOExtension\nfrom pystac_client import Client\nfrom pystac.stac_io import DefaultStacIO, StacIO\nfrom pystac.extensions.eo import Band, EOExtension\nfrom pystac.extensions.file import FileExtension\n\n\nfrom loguru import logger\nfrom urllib.parse import urljoin, urlparse\n\nimport boto3\nimport botocore\n\nfrom utils import (\n    UserSettings,\n    StarsCopyWrapper,\n    read_url,\n    ingest_items,\n    get_headers,\n    CustomStacIO,\n    getTemporalExtent,\n    getGeom,\n)\n</code></pre>"},{"location":"Describe-MLmodel/#2-create-stac-item-catalog-and-collection","title":"2) Create STAC Item, Catalog and Collection","text":"<pre><code># Create folder structure\nCATALOG_DIR = \"ML_Catalog\"\nCOLLECTION_NAME = \"ML-Models_EO\"\nITEM_ID = \"Tile-based-ML-Models\"\nSUB_DIR = os.path.join(CATALOG_DIR, COLLECTION_NAME)\n</code></pre>"},{"location":"Describe-MLmodel/#stac-item","title":"STAC Item","text":"<p>NOTE: Please execute either section 2.1) Create STAC Item or section 2.2) Upload STAC Item below according to the following:  * execute section 2.1) Create STAC Item if you want to create a STAC Item from scratch using <code>pystac</code> within this Notebook; or  * execute section 2.2) Upload STAC Item if you have already created a STAC Item (i.e. a <code>.json</code>/<code>.geojson</code> file) and want to upload it into this Notebook</p>"},{"location":"Describe-MLmodel/#21-create-stac-item","title":"2.1) Create STAC Item","text":"<pre><code># Define BBOX of the Item\nbbox = [-121.87680832296513, 36.93063805399626, -120.06532070709298, 38.84330548198025]\n\nitem = pystac.Item(\n    id=ITEM_ID,\n    bbox=bbox,\n    geometry=getGeom(bbox),\n    datetime=datetime.now(),\n    properties={},\n)\nitem\n</code></pre> <ul> <li> type \"Feature\" </li> <li> stac_version \"1.1.0\" </li> <li> stac_extensions[] 0 items </li> <li> id \"Tile-based-ML-Models\" </li> <li> geometry <ul> <li> type \"Polygon\" </li> <li> coordinates[] 1 items <ul> <li> 0[] 5 items <ul> <li> 0[] 2 items <ul> <li> 0 -121.87680832296513 </li> </ul> <ul> <li> 1 36.93063805399626 </li> </ul> </li> </ul> <ul> <li> 1[] 2 items <ul> <li> 0 -120.06532070709298 </li> </ul> <ul> <li> 1 36.93063805399626 </li> </ul> </li> </ul> <ul> <li> 2[] 2 items <ul> <li> 0 -120.06532070709298 </li> </ul> <ul> <li> 1 38.84330548198025 </li> </ul> </li> </ul> <ul> <li> 3[] 2 items <ul> <li> 0 -121.87680832296513 </li> </ul> <ul> <li> 1 38.84330548198025 </li> </ul> </li> </ul> <ul> <li> 4[] 2 items <ul> <li> 0 -121.87680832296513 </li> </ul> <ul> <li> 1 36.93063805399626 </li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li> bbox[] 4 items <ul> <li> 0 -121.87680832296513 </li> </ul> <ul> <li> 1 36.93063805399626 </li> </ul> <ul> <li> 2 -120.06532070709298 </li> </ul> <ul> <li> 3 38.84330548198025 </li> </ul> </li> <li> properties <ul> <li> datetime \"2025-04-04T12:30:39.002998Z\" </li> </ul> </li> <li> links[] 0 items </li> <li> assets <ul> </ul> </li> </ul>"},{"location":"Describe-MLmodel/#adding-item-properties","title":"Adding Item properties","text":"<p>In the following section, the user will provide the Item's properties for creation of STAC Item.</p> <pre><code># Add standard properties\nitem.properties[\"start_datetime\"] = \"2023-06-13T00:00:00Z\"\nitem.properties[\"end_datetime\"] = \"2023-06-18T23:59:59Z\"\nitem.properties[\"description\"] = (\n    \"\"\"Tile based classifier using CNNs for land cover classification. \n    The model is trained on the Sentinel-2 dataset and is capable of classifying \n    land cover types such as water, forest, urban, and agriculture. \n    The model is designed to work with Sentinel-2 imagery and can be used for \"\"\"\n)\n\nitem\n</code></pre> <ul> <li> type \"Feature\" </li> <li> stac_version \"1.1.0\" </li> <li> stac_extensions[] 0 items </li> <li> id \"Tile-based-ML-Models\" </li> <li> geometry <ul> <li> type \"Polygon\" </li> <li> coordinates[] 1 items <ul> <li> 0[] 5 items <ul> <li> 0[] 2 items <ul> <li> 0 -121.87680832296513 </li> </ul> <ul> <li> 1 36.93063805399626 </li> </ul> </li> </ul> <ul> <li> 1[] 2 items <ul> <li> 0 -120.06532070709298 </li> </ul> <ul> <li> 1 36.93063805399626 </li> </ul> </li> </ul> <ul> <li> 2[] 2 items <ul> <li> 0 -120.06532070709298 </li> </ul> <ul> <li> 1 38.84330548198025 </li> </ul> </li> </ul> <ul> <li> 3[] 2 items <ul> <li> 0 -121.87680832296513 </li> </ul> <ul> <li> 1 38.84330548198025 </li> </ul> </li> </ul> <ul> <li> 4[] 2 items <ul> <li> 0 -121.87680832296513 </li> </ul> <ul> <li> 1 36.93063805399626 </li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li> bbox[] 4 items <ul> <li> 0 -121.87680832296513 </li> </ul> <ul> <li> 1 36.93063805399626 </li> </ul> <ul> <li> 2 -120.06532070709298 </li> </ul> <ul> <li> 3 38.84330548198025 </li> </ul> </li> <li> properties <ul> <li> datetime \"2025-04-04T12:30:39.002998Z\" </li> <li> start_datetime \"2023-06-13T00:00:00Z\" </li> <li> end_datetime \"2023-06-18T23:59:59Z\" </li> <li> description \"Tile based classifier using CNNs for land cover classification.      The model is trained on the Sentinel-2 dataset and is capable of classifying      land cover types such as water, forest, urban, and agriculture.      The model is designed to work with Sentinel-2 imagery and can be used for \" </li> </ul> </li> <li> links[] 0 items </li> <li> assets <ul> </ul> </li> </ul> <pre><code># Add \"ml-model\" properties\nitem.properties[\"ml-model:type\"] = \"ml-model\"\nitem.properties[\"ml-model:learning_approach\"] = \"supervised\"\nitem.properties[\"ml-model:prediction_type\"] = \"classification\"\nitem.properties[\"ml-model:architecture\"] = \"ResNet-18\"\nitem.properties[\"ml-model:training-processor-type\"] = \"cpu\"\nitem.properties[\"ml-model:training-os\"] = \"linux\"\n</code></pre> <pre><code># Add \"mlm-ext\" properties\nitem.properties[\"mlm:name\"] = \"Tile-Based Classifier\"\nitem.properties[\"mlm:architecture\"] = \"RandomForestClassifier\"\nitem.properties[\"mlm:framework\"] = \"tensorflow\"\nitem.properties[\"mlm:framework_version\"] = \"1.4.2\"\nitem.properties[\"mlm:tasks\"] = [\"classification\"]\nitem.properties[\"mlm:compiled\"] = False\nitem.properties[\"mlm:accelerator\"] = \"amd64\"\nitem.properties[\"mlm:accelerator_constrained\"] = False\n\n# Add hyperparameters\nitem.properties[\"mlm:hyperparameters\"] = {\n    \"learning_rate\": 0.001,  # Example value\n    \"batch_size\": 32,  # Example value\n    \"number_of_epochs\": 50,  # Example value\n    \"optimizer\": \"adam\",  # Example value\n    \"momentum\": 0.9,  # Example value\n    \"dropout_rate\": 0.5,  # Example value\n    \"number_of_convolutional_layers\": 3,  # Example value\n    \"filter_size\": \"3x3\",  # Example value\n    \"number_of_filters\": 64,  # Example value\n    \"activation_function\": \"relu\",  # Example value\n    \"pooling_layers\": \"max\",  # Example value\n    \"learning_rate_scheduler\": \"step_decay\",  # Example value\n    \"l2_regularization\": 1e-4,  # Example value\n}\n\nitem.properties\n</code></pre> <pre><code>{'datetime': '2025-04-04T12:30:39.002998Z',\n 'start_datetime': '2023-06-13T00:00:00Z',\n 'end_datetime': '2023-06-18T23:59:59Z',\n 'description': 'Tile based classifier using CNNs for land cover classification. \\n    The model is trained on the Sentinel-2 dataset and is capable of classifying \\n    land cover types such as water, forest, urban, and agriculture. \\n    The model is designed to work with Sentinel-2 imagery and can be used for ',\n 'ml-model:type': 'ml-model',\n 'ml-model:learning_approach': 'supervised',\n 'ml-model:prediction_type': 'classification',\n 'ml-model:architecture': 'ResNet-18',\n 'ml-model:training-processor-type': 'cpu',\n 'ml-model:training-os': 'linux',\n 'mlm:name': 'Tile-Based Classifier',\n 'mlm:architecture': 'RandomForestClassifier',\n 'mlm:framework': 'tensorflow',\n 'mlm:framework_version': '1.4.2',\n 'mlm:tasks': ['classification'],\n 'mlm:compiled': False,\n 'mlm:accelerator': 'amd64',\n 'mlm:accelerator_constrained': False,\n 'mlm:hyperparameters': {'learning_rate': 0.001,\n  'batch_size': 32,\n  'number_of_epochs': 50,\n  'optimizer': 'adam',\n  'momentum': 0.9,\n  'dropout_rate': 0.5,\n  'number_of_convolutional_layers': 3,\n  'filter_size': '3x3',\n  'number_of_filters': 64,\n  'activation_function': 'relu',\n  'pooling_layers': 'max',\n  'learning_rate_scheduler': 'step_decay',\n  'l2_regularization': 0.0001}}\n</code></pre>"},{"location":"Describe-MLmodel/#model-inputs","title":"Model inputs","text":"<p>The properties of model inputs can be populated in the cell below</p> <pre><code># Add input and output to the properties\nitem.properties[\"mlm:input\"] = [\n    {\n        \"name\": \"EO Data\",\n        \"bands\": [\n            \"B01\",\n            \"B02\",\n            \"B03\",\n            \"B04\",\n            \"B05\",\n            \"B06\",\n            \"B07\",\n            \"B08\",\n            \"B8A\",\n            \"B09\",\n            \"B10\",\n            \"B11\",\n            \"B12\",\n        ],\n        \"input\": {\n            \"shape\": [-1, 3, 64, 64],\n            \"dim_order\": [\"batch\", \"channel\", \"height\", \"width\"],\n            \"data_type\": \"float32\",\n        },\n        \"norm_type\": \"z-score\",\n    }\n]\n</code></pre>"},{"location":"Describe-MLmodel/#model-outputs","title":"Model outputs","text":"<pre><code>class_map = {\n    \"Annual Crop\": 0,\n    \"Forest\": 1,\n    \"Herbaceous Vegetation\": 2,\n    \"Highway\": 3,\n    \"Industrial Buildings\": 4,\n    \"Pasture\": 5,\n    \"Permanent Crop\": 6,\n    \"Residential Buildings\": 7,\n    \"River\": 8,\n    \"SeaLake\": 9,\n}\n\ncolor_map = {\n    0: (34, 139, 34, 255),  # AnnualCrop: Forest Green\n    1: (0, 100, 0, 255),  # Forest: Dark Green\n    2: (144, 238, 144, 255),  # HerbaceousVegetation: Light Green\n    3: (128, 128, 128, 255),  # Highway: Gray\n    4: (169, 169, 169, 255),  # Industrial: Dark Gray\n    5: (85, 107, 47, 255),  # Pasture: Olive Green\n    6: (60, 179, 113, 255),  # PermanentCrop: Medium Sea Green\n    7: (139, 69, 19, 255),  # Residential: Saddle Brown\n    8: (30, 144, 255, 255),  # River: Dodger Blue\n    9: (0, 0, 255, 255),  # SeaLake: Blue\n}\n\ntmp_dict = []\n\nfor class_name, id in class_map.items():\n    color = color_map[id]\n    # Convert RGB to hex (without the alpha value)\n    hex_color = \"#{:02X}{:02X}{:02X}\".format(color[0], color[1], color[2])\n\n    tmp_dict.append({\n        \"name\": class_name,\n        \"value\": id,\n        \"description\": f\"{class_name} tile\",\n        \"color_hint\": hex_color.lower()[1:]  # Remove the \"#\" and convert to lowercase\n    })\nitem.properties[\"mlm:output\"] = [\n    {\n        \"name\": \"CLASSIFICATION\",\n        \"tasks\": [\"segmentation\", \"semantic-segmentation\"],\n        \"result\": {\n            \"shape\": [-1, 10980, 10980],\n            \"dim_order\": [\"batch\", \"height\", \"width\"],\n            \"data_type\": \"uint8\",\n        },\n        \"post_processing_function\": None,\n        \"classification:classes\": tmp_dict\n    }\n]\n\nitem\n</code></pre> <ul> <li> type \"Feature\" </li> <li> stac_version \"1.1.0\" </li> <li> stac_extensions[] 0 items </li> <li> id \"Tile-based-ML-Models\" </li> <li> geometry <ul> <li> type \"Polygon\" </li> <li> coordinates[] 1 items <ul> <li> 0[] 5 items <ul> <li> 0[] 2 items <ul> <li> 0 -121.87680832296513 </li> </ul> <ul> <li> 1 36.93063805399626 </li> </ul> </li> </ul> <ul> <li> 1[] 2 items <ul> <li> 0 -120.06532070709298 </li> </ul> <ul> <li> 1 36.93063805399626 </li> </ul> </li> </ul> <ul> <li> 2[] 2 items <ul> <li> 0 -120.06532070709298 </li> </ul> <ul> <li> 1 38.84330548198025 </li> </ul> </li> </ul> <ul> <li> 3[] 2 items <ul> <li> 0 -121.87680832296513 </li> </ul> <ul> <li> 1 38.84330548198025 </li> </ul> </li> </ul> <ul> <li> 4[] 2 items <ul> <li> 0 -121.87680832296513 </li> </ul> <ul> <li> 1 36.93063805399626 </li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li> bbox[] 4 items <ul> <li> 0 -121.87680832296513 </li> </ul> <ul> <li> 1 36.93063805399626 </li> </ul> <ul> <li> 2 -120.06532070709298 </li> </ul> <ul> <li> 3 38.84330548198025 </li> </ul> </li> <li> properties <ul> <li> datetime \"2025-04-04T12:30:39.002998Z\" </li> <li> start_datetime \"2023-06-13T00:00:00Z\" </li> <li> end_datetime \"2023-06-18T23:59:59Z\" </li> <li> description \"Tile based classifier using CNNs for land cover classification.      The model is trained on the Sentinel-2 dataset and is capable of classifying      land cover types such as water, forest, urban, and agriculture.      The model is designed to work with Sentinel-2 imagery and can be used for \" </li> <li> ml-model:type \"ml-model\" </li> <li> ml-model:learning_approach \"supervised\" </li> <li> ml-model:prediction_type \"classification\" </li> <li> ml-model:architecture \"ResNet-18\" </li> <li> ml-model:training-processor-type \"cpu\" </li> <li> ml-model:training-os \"linux\" </li> <li> mlm:name \"Tile-Based Classifier\" </li> <li> mlm:architecture \"RandomForestClassifier\" </li> <li> mlm:framework \"tensorflow\" </li> <li> mlm:framework_version \"1.4.2\" </li> <li> mlm:tasks[] 1 items <ul> <li> 0 \"classification\" </li> </ul> </li> <li> mlm:compiled False </li> <li> mlm:accelerator \"amd64\" </li> <li> mlm:accelerator_constrained False </li> <li> mlm:hyperparameters <ul> <li> learning_rate 0.001 </li> <li> batch_size 32 </li> <li> number_of_epochs 50 </li> <li> optimizer \"adam\" </li> <li> momentum 0.9 </li> <li> dropout_rate 0.5 </li> <li> number_of_convolutional_layers 3 </li> <li> filter_size \"3x3\" </li> <li> number_of_filters 64 </li> <li> activation_function \"relu\" </li> <li> pooling_layers \"max\" </li> <li> learning_rate_scheduler \"step_decay\" </li> <li> l2_regularization 0.0001 </li> </ul> </li> <li> mlm:input[] 1 items <ul> <li> 0 <ul> <li> name \"EO Data\" </li> <li> bands[] 13 items <ul> <li> 0 \"B01\" </li> </ul> <ul> <li> 1 \"B02\" </li> </ul> <ul> <li> 2 \"B03\" </li> </ul> <ul> <li> 3 \"B04\" </li> </ul> <ul> <li> 4 \"B05\" </li> </ul> <ul> <li> 5 \"B06\" </li> </ul> <ul> <li> 6 \"B07\" </li> </ul> <ul> <li> 7 \"B08\" </li> </ul> <ul> <li> 8 \"B8A\" </li> </ul> <ul> <li> 9 \"B09\" </li> </ul> <ul> <li> 10 \"B10\" </li> </ul> <ul> <li> 11 \"B11\" </li> </ul> <ul> <li> 12 \"B12\" </li> </ul> </li> <li> input <ul> <li> shape[] 4 items <ul> <li> 0 -1 </li> </ul> <ul> <li> 1 3 </li> </ul> <ul> <li> 2 64 </li> </ul> <ul> <li> 3 64 </li> </ul> </li> <li> dim_order[] 4 items <ul> <li> 0 \"batch\" </li> </ul> <ul> <li> 1 \"channel\" </li> </ul> <ul> <li> 2 \"height\" </li> </ul> <ul> <li> 3 \"width\" </li> </ul> </li> <li> data_type \"float32\" </li> </ul> </li> <li> norm_type \"z-score\" </li> </ul> </li> </ul> </li> <li> mlm:output[] 1 items <ul> <li> 0 <ul> <li> name \"CLASSIFICATION\" </li> <li> tasks[] 2 items <ul> <li> 0 \"segmentation\" </li> </ul> <ul> <li> 1 \"semantic-segmentation\" </li> </ul> </li> <li> result <ul> <li> shape[] 3 items <ul> <li> 0 -1 </li> </ul> <ul> <li> 1 10980 </li> </ul> <ul> <li> 2 10980 </li> </ul> </li> <li> dim_order[] 3 items <ul> <li> 0 \"batch\" </li> </ul> <ul> <li> 1 \"height\" </li> </ul> <ul> <li> 2 \"width\" </li> </ul> </li> <li> data_type \"uint8\" </li> </ul> </li> <li> post_processing_function None </li> <li> classification:classes[] 10 items <ul> <li> 0 <ul> <li> name \"Annual Crop\" </li> <li> value 0 </li> <li> description \"Annual Crop tile\" </li> <li> color_hint \"228b22\" </li> </ul> </li> </ul> <ul> <li> 1 <ul> <li> name \"Forest\" </li> <li> value 1 </li> <li> description \"Forest tile\" </li> <li> color_hint \"006400\" </li> </ul> </li> </ul> <ul> <li> 2 <ul> <li> name \"Herbaceous Vegetation\" </li> <li> value 2 </li> <li> description \"Herbaceous Vegetation tile\" </li> <li> color_hint \"90ee90\" </li> </ul> </li> </ul> <ul> <li> 3 <ul> <li> name \"Highway\" </li> <li> value 3 </li> <li> description \"Highway tile\" </li> <li> color_hint \"808080\" </li> </ul> </li> </ul> <ul> <li> 4 <ul> <li> name \"Industrial Buildings\" </li> <li> value 4 </li> <li> description \"Industrial Buildings tile\" </li> <li> color_hint \"a9a9a9\" </li> </ul> </li> </ul> <ul> <li> 5 <ul> <li> name \"Pasture\" </li> <li> value 5 </li> <li> description \"Pasture tile\" </li> <li> color_hint \"556b2f\" </li> </ul> </li> </ul> <ul> <li> 6 <ul> <li> name \"Permanent Crop\" </li> <li> value 6 </li> <li> description \"Permanent Crop tile\" </li> <li> color_hint \"3cb371\" </li> </ul> </li> </ul> <ul> <li> 7 <ul> <li> name \"Residential Buildings\" </li> <li> value 7 </li> <li> description \"Residential Buildings tile\" </li> <li> color_hint \"8b4513\" </li> </ul> </li> </ul> <ul> <li> 8 <ul> <li> name \"River\" </li> <li> value 8 </li> <li> description \"River tile\" </li> <li> color_hint \"1e90ff\" </li> </ul> </li> </ul> <ul> <li> 9 <ul> <li> name \"SeaLake\" </li> <li> value 9 </li> <li> description \"SeaLake tile\" </li> <li> color_hint \"0000ff\" </li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li> links[] 0 items </li> <li> assets <ul> </ul> </li> </ul> <p>The user will add Raster bands to the Item's properties</p> <pre><code># Add \"raster:bands\" properties\ndef add_prop_RasterBands(name, cname, nd, dt, bps, res, scale, offset, unit):\n    return {\n        \"name\": name,\n        \"common_name\": cname,\n        \"nodata\": nd,\n        \"data_type\": dt,\n        \"bits_per_sample\": bps,\n        \"spatial_resolution\": res,\n        \"scale\": scale,\n        \"offset\": offset,\n        \"unit\": unit,\n    }\n\n\ndef add_prop_RasterBands_Expression(name, cname, nd, dt, exp):\n    return {\n        \"name\": name,\n        \"common_name\": cname,\n        \"nodata\": nd,\n        \"data_type\": dt,\n        \"processing:expression\": exp,\n    }\n\n\nitem.properties[\"raster:bands\"] = [\n    add_prop_RasterBands(\n        name=\"B01\",\n        cname=\"coastal\",\n        nd=0,\n        dt=\"float32\",\n        bps=15,\n        res=60,\n        scale=0.0001,\n        offset=0,\n        unit=\"m\",\n    ),\n    add_prop_RasterBands(\n        name=\"B02\",\n        cname=\"blue\",\n        nd=0,\n        dt=\"float32\",\n        bps=15,\n        res=10,\n        scale=0.0001,\n        offset=0,\n        unit=\"m\",\n    ),\n    add_prop_RasterBands(\n        name=\"B03\",\n        cname=\"green\",\n        nd=0,\n        dt=\"float32\",\n        bps=15,\n        res=10,\n        scale=0.0001,\n        offset=0,\n        unit=\"m\",\n    ),\n    add_prop_RasterBands(\n        name=\"B04\",\n        cname=\"red\",\n        nd=0,\n        dt=\"float32\",\n        bps=15,\n        res=10,\n        scale=0.0001,\n        offset=0,\n        unit=\"m\",\n    ),\n    add_prop_RasterBands(\n        name=\"B08\",\n        cname=\"nir\",\n        nd=0,\n        dt=\"float32\",\n        bps=15,\n        res=10,\n        scale=0.0001,\n        offset=0,\n        unit=\"m\",\n    ),\n    add_prop_RasterBands(\n        name=\"B8A\",\n        cname=\"nir08\",\n        nd=0,\n        dt=\"float32\",\n        bps=15,\n        res=20,\n        scale=0.0001,\n        offset=0,\n        unit=\"m\",\n    ),\n    add_prop_RasterBands(\n        name=\"B09\",\n        cname=\"nir09\",\n        nd=0,\n        dt=\"float32\",\n        bps=15,\n        res=60,\n        scale=0.0001,\n        offset=0,\n        unit=\"m\",\n    ),\n    add_prop_RasterBands(\n        name=\"B11\",\n        cname=\"swir16\",\n        nd=0,\n        dt=\"float32\",\n        bps=15,\n        res=20,\n        scale=0.0001,\n        offset=0,\n        unit=\"m\",\n    ),\n    add_prop_RasterBands(\n        name=\"B12\",\n        cname=\"swir22\",\n        nd=0,\n        dt=\"float32\",\n        bps=15,\n        res=20,\n        scale=0.0001,\n        offset=0,\n        unit=\"m\",\n    ),\n]\n\n\n# Display\nitem.properties[\"raster:bands\"]\n</code></pre> <pre><code>[{'name': 'B01',\n  'common_name': 'coastal',\n  'nodata': 0,\n  'data_type': 'float32',\n  'bits_per_sample': 15,\n  'spatial_resolution': 60,\n  'scale': 0.0001,\n  'offset': 0,\n  'unit': 'm'},\n {'name': 'B02',\n  'common_name': 'blue',\n  'nodata': 0,\n  'data_type': 'float32',\n  'bits_per_sample': 15,\n  'spatial_resolution': 10,\n  'scale': 0.0001,\n  'offset': 0,\n  'unit': 'm'},\n {'name': 'B03',\n  'common_name': 'green',\n  'nodata': 0,\n  'data_type': 'float32',\n  'bits_per_sample': 15,\n  'spatial_resolution': 10,\n  'scale': 0.0001,\n  'offset': 0,\n  'unit': 'm'},\n {'name': 'B04',\n  'common_name': 'red',\n  'nodata': 0,\n  'data_type': 'float32',\n  'bits_per_sample': 15,\n  'spatial_resolution': 10,\n  'scale': 0.0001,\n  'offset': 0,\n  'unit': 'm'},\n {'name': 'B08',\n  'common_name': 'nir',\n  'nodata': 0,\n  'data_type': 'float32',\n  'bits_per_sample': 15,\n  'spatial_resolution': 10,\n  'scale': 0.0001,\n  'offset': 0,\n  'unit': 'm'},\n {'name': 'B8A',\n  'common_name': 'nir08',\n  'nodata': 0,\n  'data_type': 'float32',\n  'bits_per_sample': 15,\n  'spatial_resolution': 20,\n  'scale': 0.0001,\n  'offset': 0,\n  'unit': 'm'},\n {'name': 'B09',\n  'common_name': 'nir09',\n  'nodata': 0,\n  'data_type': 'float32',\n  'bits_per_sample': 15,\n  'spatial_resolution': 60,\n  'scale': 0.0001,\n  'offset': 0,\n  'unit': 'm'},\n {'name': 'B11',\n  'common_name': 'swir16',\n  'nodata': 0,\n  'data_type': 'float32',\n  'bits_per_sample': 15,\n  'spatial_resolution': 20,\n  'scale': 0.0001,\n  'offset': 0,\n  'unit': 'm'},\n {'name': 'B12',\n  'common_name': 'swir22',\n  'nodata': 0,\n  'data_type': 'float32',\n  'bits_per_sample': 15,\n  'spatial_resolution': 20,\n  'scale': 0.0001,\n  'offset': 0,\n  'unit': 'm'}]\n</code></pre> <pre><code># Add Assets - ML Training\napp_version = \"0.0.2\"\nasset = pystac.Asset(\n    title=\"Workflow for tile-based training\",\n    href=f\"https://github.com/parham-membari-terradue/machine-learning-process/releases/download/{app_version}/tile-sat-training.{app_version}.cwl\",\n    media_type=\"application/cwl+yaml\",\n    roles=[\"ml-model:training-runtime\", \"runtime\", \"mlm:training-runtime\"],\n)\nitem.add_asset(\"tile-based-training\", asset)\n\n# Add Assets - Inference\nasset = pystac.Asset(\n    title=\"Workflow for tile-based inference\",\n    href=f\"https://github.com/parham-membari-terradue/machine-learning-process/releases/download/{app_version}/tile-sat-inference.{app_version}.cwl\",\n    media_type=\"application/cwl+yaml\",\n    roles=[\"ml-model:inference-runtime\", \"runtime\", \"mlm:inference-runtime\"],\n)\nitem.add_asset(\"tile-based-inference\", asset)\n\n# Add Asset - ML model\nasset = pystac.Asset(\n    title=\"ONNX Model\",\n    href=\"https://github.com/parham-membari-terradue/machine-learning-process/blob/main/inference/make-inference/src/make_inference/model/model.onnx\",\n    media_type=\"application/octet-stream; framework=onnx; profile=onnx\",\n    roles=[\"mlm:model\"],\n)\nitem.add_asset(\"model\", asset)\n\nitem.assets\n</code></pre> <pre><code>{'tile-based-training': &lt;Asset href=https://github.com/parham-membari-terradue/machine-learning-process/releases/download/0.0.2/tile-sat-training.0.0.2.cwl&gt;,\n 'tile-based-inference': &lt;Asset href=https://github.com/parham-membari-terradue/machine-learning-process/releases/download/0.0.2/tile-sat-inference.0.0.2.cwl&gt;,\n 'model': &lt;Asset href=https://github.com/parham-membari-terradue/machine-learning-process/blob/main/inference/make-inference/src/make_inference/model/model.onnx&gt;}\n</code></pre> <pre><code># Add links\nrel_path = f\"./{SUB_DIR}/{item.id}/{item.id}.json\"\nitem.set_self_href(rel_path)\nitem.links\n</code></pre> <pre><code>[&lt;Link rel=self target=/home/t2/Desktop/p/argo/machine-learning-process/MLM/ML_Catalog/ML-Models_EO/Tile-based-ML-Models/Tile-based-ML-Models.json&gt;]\n</code></pre> <p>In addition to the <code>EO</code> STAC Extension, the user can add the \"ML Model\" STAC Extension (<code>ml-model</code>) in the STAC Item. </p> <p>There is an upcoming extension for ML models that is under development, which will allow to store more details and information related to the ML model: \"Machine Learning Model\" STAC Extension (<code>mlm</code>). </p> <pre><code># Add Extensions\nEOExtension.ext(item, add_if_missing=True)\n\n# Add the extension to the item and set the schema URL\nif not any(\"ml-model\" in url for url in item.stac_extensions):\n    item.stac_extensions.append(\n        \"https://stac-extensions.github.io/ml-model/v1.0.0/schema.json\"\n    )\nif not any(\"mlm-extension\" in url for url in item.stac_extensions):\n    item.stac_extensions.append(\n        \"https://crim-ca.github.io/mlm-extension/v1.2.0/schema.json\"\n    )\nif not any(\"raster\" in url for url in item.stac_extensions):\n    item.stac_extensions.append(\n        \"https://stac-extensions.github.io/raster/v1.1.0/schema.json\"\n    )\nif not any(\"file\" in url for url in item.stac_extensions):\n    item.stac_extensions.append(\n        \"https://stac-extensions.github.io/file/v2.1.0/schema.json\"\n    )\nitem.stac_extensions\n</code></pre> <pre><code>['https://stac-extensions.github.io/eo/v1.1.0/schema.json',\n 'https://stac-extensions.github.io/ml-model/v1.0.0/schema.json',\n 'https://crim-ca.github.io/mlm-extension/v1.2.0/schema.json',\n 'https://stac-extensions.github.io/raster/v1.1.0/schema.json',\n 'https://stac-extensions.github.io/file/v2.1.0/schema.json']\n</code></pre> <pre><code>item\n</code></pre> <ul> <li> type \"Feature\" </li> <li> stac_version \"1.1.0\" </li> <li> stac_extensions[] 5 items <ul> <li> 0 \"https://stac-extensions.github.io/eo/v1.1.0/schema.json\" </li> </ul> <ul> <li> 1 \"https://stac-extensions.github.io/ml-model/v1.0.0/schema.json\" </li> </ul> <ul> <li> 2 \"https://crim-ca.github.io/mlm-extension/v1.2.0/schema.json\" </li> </ul> <ul> <li> 3 \"https://stac-extensions.github.io/raster/v1.1.0/schema.json\" </li> </ul> <ul> <li> 4 \"https://stac-extensions.github.io/file/v2.1.0/schema.json\" </li> </ul> </li> <li> id \"Tile-based-ML-Models\" </li> <li> geometry <ul> <li> type \"Polygon\" </li> <li> coordinates[] 1 items <ul> <li> 0[] 5 items <ul> <li> 0[] 2 items <ul> <li> 0 -121.87680832296513 </li> </ul> <ul> <li> 1 36.93063805399626 </li> </ul> </li> </ul> <ul> <li> 1[] 2 items <ul> <li> 0 -120.06532070709298 </li> </ul> <ul> <li> 1 36.93063805399626 </li> </ul> </li> </ul> <ul> <li> 2[] 2 items <ul> <li> 0 -120.06532070709298 </li> </ul> <ul> <li> 1 38.84330548198025 </li> </ul> </li> </ul> <ul> <li> 3[] 2 items <ul> <li> 0 -121.87680832296513 </li> </ul> <ul> <li> 1 38.84330548198025 </li> </ul> </li> </ul> <ul> <li> 4[] 2 items <ul> <li> 0 -121.87680832296513 </li> </ul> <ul> <li> 1 36.93063805399626 </li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li> bbox[] 4 items <ul> <li> 0 -121.87680832296513 </li> </ul> <ul> <li> 1 36.93063805399626 </li> </ul> <ul> <li> 2 -120.06532070709298 </li> </ul> <ul> <li> 3 38.84330548198025 </li> </ul> </li> <li> properties <ul> <li> datetime \"2025-04-04T12:30:39.002998Z\" </li> <li> start_datetime \"2023-06-13T00:00:00Z\" </li> <li> end_datetime \"2023-06-18T23:59:59Z\" </li> <li> description \"Tile based classifier using CNNs for land cover classification.      The model is trained on the Sentinel-2 dataset and is capable of classifying      land cover types such as water, forest, urban, and agriculture.      The model is designed to work with Sentinel-2 imagery and can be used for \" </li> <li> ml-model:type \"ml-model\" </li> <li> ml-model:learning_approach \"supervised\" </li> <li> ml-model:prediction_type \"classification\" </li> <li> ml-model:architecture \"ResNet-18\" </li> <li> ml-model:training-processor-type \"cpu\" </li> <li> ml-model:training-os \"linux\" </li> <li> mlm:name \"Tile-Based Classifier\" </li> <li> mlm:architecture \"RandomForestClassifier\" </li> <li> mlm:framework \"tensorflow\" </li> <li> mlm:framework_version \"1.4.2\" </li> <li> mlm:tasks[] 1 items <ul> <li> 0 \"classification\" </li> </ul> </li> <li> mlm:compiled False </li> <li> mlm:accelerator \"amd64\" </li> <li> mlm:accelerator_constrained False </li> <li> mlm:hyperparameters <ul> <li> learning_rate 0.001 </li> <li> batch_size 32 </li> <li> number_of_epochs 50 </li> <li> optimizer \"adam\" </li> <li> momentum 0.9 </li> <li> dropout_rate 0.5 </li> <li> number_of_convolutional_layers 3 </li> <li> filter_size \"3x3\" </li> <li> number_of_filters 64 </li> <li> activation_function \"relu\" </li> <li> pooling_layers \"max\" </li> <li> learning_rate_scheduler \"step_decay\" </li> <li> l2_regularization 0.0001 </li> </ul> </li> <li> mlm:input[] 1 items <ul> <li> 0 <ul> <li> name \"EO Data\" </li> <li> bands[] 13 items <ul> <li> 0 \"B01\" </li> </ul> <ul> <li> 1 \"B02\" </li> </ul> <ul> <li> 2 \"B03\" </li> </ul> <ul> <li> 3 \"B04\" </li> </ul> <ul> <li> 4 \"B05\" </li> </ul> <ul> <li> 5 \"B06\" </li> </ul> <ul> <li> 6 \"B07\" </li> </ul> <ul> <li> 7 \"B08\" </li> </ul> <ul> <li> 8 \"B8A\" </li> </ul> <ul> <li> 9 \"B09\" </li> </ul> <ul> <li> 10 \"B10\" </li> </ul> <ul> <li> 11 \"B11\" </li> </ul> <ul> <li> 12 \"B12\" </li> </ul> </li> <li> input <ul> <li> shape[] 4 items <ul> <li> 0 -1 </li> </ul> <ul> <li> 1 3 </li> </ul> <ul> <li> 2 64 </li> </ul> <ul> <li> 3 64 </li> </ul> </li> <li> dim_order[] 4 items <ul> <li> 0 \"batch\" </li> </ul> <ul> <li> 1 \"channel\" </li> </ul> <ul> <li> 2 \"height\" </li> </ul> <ul> <li> 3 \"width\" </li> </ul> </li> <li> data_type \"float32\" </li> </ul> </li> <li> norm_type \"z-score\" </li> </ul> </li> </ul> </li> <li> mlm:output[] 1 items <ul> <li> 0 <ul> <li> name \"CLASSIFICATION\" </li> <li> tasks[] 2 items <ul> <li> 0 \"segmentation\" </li> </ul> <ul> <li> 1 \"semantic-segmentation\" </li> </ul> </li> <li> result <ul> <li> shape[] 3 items <ul> <li> 0 -1 </li> </ul> <ul> <li> 1 10980 </li> </ul> <ul> <li> 2 10980 </li> </ul> </li> <li> dim_order[] 3 items <ul> <li> 0 \"batch\" </li> </ul> <ul> <li> 1 \"height\" </li> </ul> <ul> <li> 2 \"width\" </li> </ul> </li> <li> data_type \"uint8\" </li> </ul> </li> <li> post_processing_function None </li> <li> classification:classes[] 10 items <ul> <li> 0 <ul> <li> name \"Annual Crop\" </li> <li> value 0 </li> <li> description \"Annual Crop tile\" </li> <li> color_hint \"228b22\" </li> </ul> </li> </ul> <ul> <li> 1 <ul> <li> name \"Forest\" </li> <li> value 1 </li> <li> description \"Forest tile\" </li> <li> color_hint \"006400\" </li> </ul> </li> </ul> <ul> <li> 2 <ul> <li> name \"Herbaceous Vegetation\" </li> <li> value 2 </li> <li> description \"Herbaceous Vegetation tile\" </li> <li> color_hint \"90ee90\" </li> </ul> </li> </ul> <ul> <li> 3 <ul> <li> name \"Highway\" </li> <li> value 3 </li> <li> description \"Highway tile\" </li> <li> color_hint \"808080\" </li> </ul> </li> </ul> <ul> <li> 4 <ul> <li> name \"Industrial Buildings\" </li> <li> value 4 </li> <li> description \"Industrial Buildings tile\" </li> <li> color_hint \"a9a9a9\" </li> </ul> </li> </ul> <ul> <li> 5 <ul> <li> name \"Pasture\" </li> <li> value 5 </li> <li> description \"Pasture tile\" </li> <li> color_hint \"556b2f\" </li> </ul> </li> </ul> <ul> <li> 6 <ul> <li> name \"Permanent Crop\" </li> <li> value 6 </li> <li> description \"Permanent Crop tile\" </li> <li> color_hint \"3cb371\" </li> </ul> </li> </ul> <ul> <li> 7 <ul> <li> name \"Residential Buildings\" </li> <li> value 7 </li> <li> description \"Residential Buildings tile\" </li> <li> color_hint \"8b4513\" </li> </ul> </li> </ul> <ul> <li> 8 <ul> <li> name \"River\" </li> <li> value 8 </li> <li> description \"River tile\" </li> <li> color_hint \"1e90ff\" </li> </ul> </li> </ul> <ul> <li> 9 <ul> <li> name \"SeaLake\" </li> <li> value 9 </li> <li> description \"SeaLake tile\" </li> <li> color_hint \"0000ff\" </li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li> raster:bands[] 9 items <ul> <li> 0 <ul> <li> name \"B01\" </li> <li> common_name \"coastal\" </li> <li> nodata 0 </li> <li> data_type \"float32\" </li> <li> bits_per_sample 15 </li> <li> spatial_resolution 60 </li> <li> scale 0.0001 </li> <li> offset 0 </li> <li> unit \"m\" </li> </ul> </li> </ul> <ul> <li> 1 <ul> <li> name \"B02\" </li> <li> common_name \"blue\" </li> <li> nodata 0 </li> <li> data_type \"float32\" </li> <li> bits_per_sample 15 </li> <li> spatial_resolution 10 </li> <li> scale 0.0001 </li> <li> offset 0 </li> <li> unit \"m\" </li> </ul> </li> </ul> <ul> <li> 2 <ul> <li> name \"B03\" </li> <li> common_name \"green\" </li> <li> nodata 0 </li> <li> data_type \"float32\" </li> <li> bits_per_sample 15 </li> <li> spatial_resolution 10 </li> <li> scale 0.0001 </li> <li> offset 0 </li> <li> unit \"m\" </li> </ul> </li> </ul> <ul> <li> 3 <ul> <li> name \"B04\" </li> <li> common_name \"red\" </li> <li> nodata 0 </li> <li> data_type \"float32\" </li> <li> bits_per_sample 15 </li> <li> spatial_resolution 10 </li> <li> scale 0.0001 </li> <li> offset 0 </li> <li> unit \"m\" </li> </ul> </li> </ul> <ul> <li> 4 <ul> <li> name \"B08\" </li> <li> common_name \"nir\" </li> <li> nodata 0 </li> <li> data_type \"float32\" </li> <li> bits_per_sample 15 </li> <li> spatial_resolution 10 </li> <li> scale 0.0001 </li> <li> offset 0 </li> <li> unit \"m\" </li> </ul> </li> </ul> <ul> <li> 5 <ul> <li> name \"B8A\" </li> <li> common_name \"nir08\" </li> <li> nodata 0 </li> <li> data_type \"float32\" </li> <li> bits_per_sample 15 </li> <li> spatial_resolution 20 </li> <li> scale 0.0001 </li> <li> offset 0 </li> <li> unit \"m\" </li> </ul> </li> </ul> <ul> <li> 6 <ul> <li> name \"B09\" </li> <li> common_name \"nir09\" </li> <li> nodata 0 </li> <li> data_type \"float32\" </li> <li> bits_per_sample 15 </li> <li> spatial_resolution 60 </li> <li> scale 0.0001 </li> <li> offset 0 </li> <li> unit \"m\" </li> </ul> </li> </ul> <ul> <li> 7 <ul> <li> name \"B11\" </li> <li> common_name \"swir16\" </li> <li> nodata 0 </li> <li> data_type \"float32\" </li> <li> bits_per_sample 15 </li> <li> spatial_resolution 20 </li> <li> scale 0.0001 </li> <li> offset 0 </li> <li> unit \"m\" </li> </ul> </li> </ul> <ul> <li> 8 <ul> <li> name \"B12\" </li> <li> common_name \"swir22\" </li> <li> nodata 0 </li> <li> data_type \"float32\" </li> <li> bits_per_sample 15 </li> <li> spatial_resolution 20 </li> <li> scale 0.0001 </li> <li> offset 0 </li> <li> unit \"m\" </li> </ul> </li> </ul> </li> </ul> </li> <li> links[] 1 items <ul> <li> 0 <ul> <li> rel \"self\" </li> <li> href \"/home/t2/Desktop/p/argo/machine-learning-process/MLM/ML_Catalog/ML-Models_EO/Tile-based-ML-Models/Tile-based-ML-Models.json\" </li> <li> type \"application/json\" </li> </ul> </li> </ul> </li> <li> assets <ul> <li> tile-based-training <ul> <li> href \"https://github.com/parham-membari-terradue/machine-learning-process/releases/download/0.0.2/tile-sat-training.0.0.2.cwl\" </li> <li> type \"application/cwl+yaml\" </li> <li> title \"Workflow for tile-based training\" </li> <li> roles[] 3 items <ul> <li> 0 \"ml-model:training-runtime\" </li> </ul> <ul> <li> 1 \"runtime\" </li> </ul> <ul> <li> 2 \"mlm:training-runtime\" </li> </ul> </li> </ul> </li> <li> tile-based-inference <ul> <li> href \"https://github.com/parham-membari-terradue/machine-learning-process/releases/download/0.0.2/tile-sat-inference.0.0.2.cwl\" </li> <li> type \"application/cwl+yaml\" </li> <li> title \"Workflow for tile-based inference\" </li> <li> roles[] 3 items <ul> <li> 0 \"ml-model:inference-runtime\" </li> </ul> <ul> <li> 1 \"runtime\" </li> </ul> <ul> <li> 2 \"mlm:inference-runtime\" </li> </ul> </li> </ul> </li> <li> model <ul> <li> href \"https://github.com/parham-membari-terradue/machine-learning-process/blob/main/inference/make-inference/src/make_inference/model/model.onnx\" </li> <li> type \"application/octet-stream; framework=onnx; profile=onnx\" </li> <li> title \"ONNX Model\" </li> <li> roles[] 1 items <ul> <li> 0 \"mlm:model\" </li> </ul> </li> </ul> </li> </ul> </li> </ul> <pre><code># Validate STAC Item\nitem.validate()\n</code></pre> <pre><code>['https://schemas.stacspec.org/v1.1.0/item-spec/json-schema/item.json',\n 'https://stac-extensions.github.io/eo/v1.1.0/schema.json',\n 'https://stac-extensions.github.io/ml-model/v1.0.0/schema.json',\n 'https://crim-ca.github.io/mlm-extension/v1.2.0/schema.json',\n 'https://stac-extensions.github.io/raster/v1.1.0/schema.json',\n 'https://stac-extensions.github.io/file/v2.1.0/schema.json']\n</code></pre>"},{"location":"Describe-MLmodel/#23-stac-objects","title":"2.3) STAC Objects","text":""},{"location":"Describe-MLmodel/#stac-catalog","title":"STAC Catalog","text":"<p>Check if a <code>catalog.json</code> exists already. If not, create it, otherwise read existing catalog and add STAC Item to it.</p> <pre><code>cat_path = os.path.join(CATALOG_DIR, \"catalog.json\")\n\nif not os.path.exists(cat_path):\n    # Catalog does not exist - create it\n    print(\"Catalog does not exist. Creating it\")\n\n    catalog = pystac.Catalog(\n        id=\"ML-Model_EO\", description=\"A catalog to describe ML models\", title=\"ML Models\"\n    )\nelse:\n    # Read Catalog and add the STAC Item to it\n    print(\"Catalog exists already. Reading it\")\n\n    catalog = pystac.read_file(cat_path)\ncatalog.validate()\ncatalog\n</code></pre> <pre><code>Catalog does not exist. Creating it\n</code></pre> <ul> <li> type \"Catalog\" </li> <li> id \"ML-Model_EO\" </li> <li> stac_version \"1.1.0\" </li> <li> description \"A catalog to describe ML models\" </li> <li> links[] 0 items </li> <li> title \"ML Models\" </li> </ul>"},{"location":"Describe-MLmodel/#stac-collection","title":"STAC Collection","text":"<p>Check if a <code>collection.json</code> exists already. If not, create it, otherwise read existing collection and add STAC Item to it.</p> <pre><code>coll_path = os.path.join(CATALOG_DIR, COLLECTION_NAME, \"collection.json\")\n\nif not os.path.exists(coll_path):\n    # Collection does not exist - create it\n    print(\"Collection does not exist. Creating it\")\n\n    # Spatial extent\n    bbox_world = [-180, -90, 180, 90]\n    # Define temporal extent\n    start_date = \"2015-06-27T00:00:01.000000+00:00\"\n    end_date = None  # \"2024-04-29T13:23:32.741484+00:00\"\n\n    collection = pystac.Collection(\n        id=COLLECTION_NAME,\n        description=\"A collection for ML Models\",\n        extent=pystac.Extent(\n            spatial=pystac.SpatialExtent(bbox_world),\n            temporal=getTemporalExtent(start_date, end_date),\n        ),\n        title=COLLECTION_NAME,\n        license=\"proprietary\",\n        keywords=[],\n        providers=[\n            pystac.Provider(\n                name=\"AI-Extensions Project\",\n                roles=[\"producer\"],\n                url=\"https://ai-extensions.github.io/docs\",\n            )\n        ],\n    )\n\nelse:\n    # Read Collection and add the STAC Item to it\n    print(\"Collection exists already. Reading it\")\n\n    collection = read_file(coll_path)\n\ncollection\n</code></pre> <pre><code>Collection does not exist. Creating it\n</code></pre> <ul> <li> type \"Collection\" </li> <li> id \"ML-Models_EO\" </li> <li> stac_version \"1.1.0\" </li> <li> description \"A collection for ML Models\" </li> <li> links[] 0 items </li> <li> title \"ML-Models_EO\" </li> <li> extent <ul> <li> spatial <ul> <li> bbox[] 1 items <ul> <li> 0[] 4 items <ul> <li> 0 -180 </li> </ul> <ul> <li> 1 -90 </li> </ul> <ul> <li> 2 180 </li> </ul> <ul> <li> 3 90 </li> </ul> </li> </ul> </li> </ul> </li> <li> temporal <ul> <li> interval[] 1 items <ul> <li> 0[] 2 items <ul> <li> 0 \"2015-06-27T00:00:01Z\" </li> </ul> <ul> <li> 1 None </li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li> license \"proprietary\" </li> <li> providers[] 1 items <ul> <li> 0 <ul> <li> name \"AI-Extensions Project\" </li> <li> roles[] 1 items <ul> <li> 0 \"producer\" </li> </ul> </li> <li> url \"https://ai-extensions.github.io/docs\" </li> </ul> </li> </ul> </li> </ul>"},{"location":"Describe-MLmodel/#interlink-stac-objects","title":"Interlink STAC Objects","text":"<p>Note: In order to add a STAC Item to the Collection, ensure that a STAC Item is not already present in the Collection. </p> <p>If that's the case, firstly open the <code>collection.json</code> file and delete the Item from it, then open the <code>catalog.json</code> file and delete the Collection from it. </p> <p>This will ensure that both <code>collection.json</code> and <code>catalog.json</code> files are updated correctly.  </p> <pre><code># Add STAC Item to the Collection. Note: this works only if there are no items in the collection\nif not any(item.id in link.href for link in collection.links if link.rel == \"item\"):\n    # Add item\n    print(\"Adding item\")\n    collection.add_item(item=item)\ncollection\n</code></pre> <pre><code>Adding item\n</code></pre> <ul> <li> type \"Collection\" </li> <li> id \"ML-Models_EO\" </li> <li> stac_version \"1.1.0\" </li> <li> description \"A collection for ML Models\" </li> <li> links[] 1 items <ul> <li> 0 <ul> <li> rel \"item\" </li> <li> href \"/home/t2/Desktop/p/argo/machine-learning-process/MLM/ML_Catalog/ML-Models_EO/Tile-based-ML-Models/Tile-based-ML-Models.json\" </li> <li> type \"application/geo+json\" </li> </ul> </li> </ul> </li> <li> title \"ML-Models_EO\" </li> <li> extent <ul> <li> spatial <ul> <li> bbox[] 1 items <ul> <li> 0[] 4 items <ul> <li> 0 -180 </li> </ul> <ul> <li> 1 -90 </li> </ul> <ul> <li> 2 180 </li> </ul> <ul> <li> 3 90 </li> </ul> </li> </ul> </li> </ul> </li> <li> temporal <ul> <li> interval[] 1 items <ul> <li> 0[] 2 items <ul> <li> 0 \"2015-06-27T00:00:01Z\" </li> </ul> <ul> <li> 1 None </li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li> license \"proprietary\" </li> <li> providers[] 1 items <ul> <li> 0 <ul> <li> name \"AI-Extensions Project\" </li> <li> roles[] 1 items <ul> <li> 0 \"producer\" </li> </ul> </li> <li> url \"https://ai-extensions.github.io/docs\" </li> </ul> </li> </ul> </li> </ul> <pre><code># Add Collection to the Catalog.\nprint(\"Adding Collection\")\ncollection.set_parent(catalog)\ncatalog.add_child(collection)\ncatalog\n</code></pre> <pre><code>Adding Collection\n</code></pre> <ul> <li> type \"Catalog\" </li> <li> id \"ML-Model_EO\" </li> <li> stac_version \"1.1.0\" </li> <li> description \"A catalog to describe ML models\" </li> <li> links[] 1 items <ul> <li> 0 <ul> <li> rel \"child\" </li> <li> href None </li> <li> type \"application/json\" </li> <li> title \"ML-Models_EO\" </li> </ul> </li> </ul> </li> <li> title \"ML Models\" </li> </ul> <p>Normalise the catalog to save the files under a specific folder name</p> <pre><code>catalog.normalize_and_save(\n    root_href=CATALOG_DIR, catalog_type=pystac.CatalogType.SELF_CONTAINED\n)\ncatalog.validate()\ncatalog\n</code></pre> <ul> <li> type \"Catalog\" </li> <li> id \"ML-Model_EO\" </li> <li> stac_version \"1.1.0\" </li> <li> description \"A catalog to describe ML models\" </li> <li> links[] 3 items <ul> <li> 0 <ul> <li> rel \"root\" </li> <li> href \"/home/t2/Desktop/p/argo/machine-learning-process/MLM/ML_Catalog/catalog.json\" </li> <li> type \"application/json\" </li> <li> title \"ML Models\" </li> </ul> </li> </ul> <ul> <li> 1 <ul> <li> rel \"child\" </li> <li> href \"/home/t2/Desktop/p/argo/machine-learning-process/MLM/ML_Catalog/ML-Models_EO/collection.json\" </li> <li> type \"application/json\" </li> <li> title \"ML-Models_EO\" </li> </ul> </li> </ul> <ul> <li> 2 <ul> <li> rel \"self\" </li> <li> href \"/home/t2/Desktop/p/argo/machine-learning-process/MLM/ML_Catalog/catalog.json\" </li> <li> type \"application/json\" </li> </ul> </li> </ul> </li> <li> title \"ML Models\" </li> </ul> <pre><code># Check that collection and item have been included in the catalog\ncatalog.describe()\n</code></pre> <pre><code>* &lt;Catalog id=ML-Model_EO&gt;\n    * &lt;Collection id=ML-Models_EO&gt;\n      * &lt;Item id=Tile-based-ML-Models&gt;\n</code></pre>"},{"location":"extract-model/","title":"Export the Best Model to ONNX Format","text":"<p>This notebook provides a step-by-step tutorial for exporting a selected model from the MLflow model registry to ONNX format. The converted model is saved within the inference Python module to support the development of a new Python application and the creation of an inference Docker image, which is then published to the designated container registry. </p> <p>Note: This process has already been completed. However, users may need to repeat it with their own candidate models.</p>"},{"location":"extract-model/#install-dependencies","title":"Install dependencies","text":"<pre><code>pip install tf2onnx onnxmltools onnxruntime onnx mlflow tensorflow\n</code></pre>"},{"location":"extract-model/#import-dependencies","title":"Import dependencies","text":"<pre><code>import json\nimport os\nimport mlflow\nimport tensorflow as tf\nimport tf2onnx\nimport keras\n</code></pre>"},{"location":"extract-model/#save-model-in-onnx-format","title":"Save Model in ONNX Format","text":"<p>In the cells below, the user will download the best model artifact from the MLflow model registry and then save it in the ONNX format.</p> <p>Note: You may need to decrease the <code>desired_test_accuracy</code> to find active runs in the MLflow model registry.</p> <pre><code>params = {\n    \"MLFLOW_TRACKING_URI\": \"http://localhost:5000/\",\n    \"experiment_id\": \"EuroSAT_classification\",\n\n}\ndesired_test_accuracy = 0.85\n</code></pre> <pre><code># Search for best run\nactive_runs = (\n    mlflow.search_runs(\n        experiment_names=[params[\"experiment_id\"]],\n        filter_string=f\"metrics.test_accuracy &gt; {desired_test_accuracy}\",\n        search_all_experiments=True,\n    )\n    .sort_values(by=[\"metrics.test_accuracy\", \"metrics.test_precision\"], ascending=False)\n    .reset_index()\n    .loc[0]\n)\nrun_id = active_runs[\"run_id\"]\nprint(f\"Selected run_id: {run_id}\")\n\n# Download just the .keras file\nmodel_uri = f\"runs:/{run_id}/model/model.keras/data/model.keras\"\nkeras_path = mlflow.artifacts.download_artifacts(artifact_uri=model_uri)\nprint(f\"Downloaded Keras file path: {keras_path}\")\n\n# Load the Keras v3 model\nkeras_model = keras.models.load_model(keras_path)\n\n# Define input signature\ninput_signature = [tf.TensorSpec([None, 64, 64, 12], tf.float32, name=\"input\")]\n\n@tf.function(input_signature=input_signature)\ndef model_func(x):\n    return keras_model(x)\n\n# Convert to ONNX\nonnx_model, _ = tf2onnx.convert.from_function(\n    model_func,\n    input_signature=input_signature,\n    opset=13,\n    output_path=\"model.onnx\"\n)\n\nprint(\"\u2705 Successfully saved model.onnx\")\n</code></pre>"},{"location":"hands-on/","title":"Hands-on","text":"<p>This page provides a step-by-step guide to help you run different components of the workflow:</p>"},{"location":"hands-on/#tile-based-training-implementation","title":"Tile-based Training Implementation","text":"<p>Refer to the training component documentation and run multiple training jobs with various model hyperparameters.</p>"},{"location":"hands-on/#tile-based-inference-implementation","title":"Tile-based Inference Implementation","text":"<p>Check the inference component documentation and run inference using different Sentinel-2 products in parallel using <code>calrissian</code>, or once at the time using <code>cwltool</code>.</p>"},{"location":"hands-on/#describing-the-ml-model","title":"Describing the ML Model","text":"<p>Refer to the MLM component documentation and execute the provided notebook to describe the machine learning model.</p>"},{"location":"inference-container/","title":"Inference container:","text":"<p>This module enables users to create an inference pipeline that takes a Sentinel-2 STAC Item from the Planetary Computer, and generates a binary mask TIFF image using a pre-trained CNN model. For details on how the model was trained, refer to the training container documentation.</p>"},{"location":"inference-container/#make-inference-module","title":"<code>Make Inference</code> Module:","text":"<p>Inputs:</p> <ul> <li><code>input_reference</code>: A list of Sentinel-2 product references from Planetary Computer. Note: the inference application provides accurate results only when the Sentinel-2 product has low or no cloud cover. High cloud coverage may significantly reduce prediction accuracy.</li> </ul> <p>Outputs:</p> <ul> <li><code>{STAC_ITEM_ID}_classified.tif</code>: A binary <code>.tif</code> image in <code>COG</code> format containing the full-resolution land cover classification predicted by the model, with each pixel assigned to a land cover class as defined in the table below. </li> <li><code>overview_{STAC_ITEM_ID}_classified.tif</code>: A binary <code>.tif</code> image in <code>COG</code> format containing lower-resolution overview of the classification result, generated to support fast visualisation and efficient browsing across zoom levels. </li> <li><code>STAC objects</code>: STAC objects related to the provided masks, including STAC Catalog and STAC Item.</li> </ul> <p>Land Cover Classes | Class ID | Class Name            | |----------|-----------------------| | 0        | AnnualCrop            | | 1        | Forest                | | 2        | HerbaceousVegetation  | | 3        | Highway               | | 4        | Industrial            | | 5        | Pasture               | | 6        | PermanentCrop         | | 7        | Residential           | | 8        | River                 | | 9        | SeaLake               | | 10       | No Data               |</p>"},{"location":"inference-container/#how-the-application-works","title":"How the Application Works","text":"<p>The application begins by reading the input Sentinel-2 STAC Item(s) from the Planetary Computer and then extracting the 12 common Sentinel-2 spectral bands (see table below), ordered to match those expected by the trained ML model. </p> <p>Sentinel-2 Spectral Bands | Index | Asset Key  | Asset Common Name | |-------|------------|-------------------| | 1     | B01        | Coastal           | | 2     | B02        | Blue              | | 3     | B03        | Green             | | 4     | B04        | Red               | | 5     | B05        | Red Edge          | | 6     | B06        | Red Edge          | | 7     | B07        | Red Edge          | | 8     | B08        | NIR               | | 9     | B8A        | Narrow NIR        | | 10    | B09        | Water Vapor       | | 11    | B11        | SWIR 1 (16)       | | 12    | B12        | SWIR 2 (22)       |</p> <p>As part of the preprocessing, all selected bands are resampled to a consistent spatial resolution of 10 meters.</p> <p>The pipeline then proceeds with a sliding window approach: it reads and stacks small image chips from the resampled bands (in the specified order), forming multi-band input arrays. These image chips are fed to the trained CNN model, which predicts the corresponding LC class for each chip.</p> <p>At the end of the process, the application generates: - The LC classification prediction map (COG mask) - A visual overview image - An updated STAC Catalog and Item containing metadata and references to the output files.</p>"},{"location":"inference-cwl/","title":"Inference Module &amp; CWL Runner","text":"<p>In the training module, a CNN model was trained on the EuroSAT dataset to classify image chips into 10 different land use/land cover classes. The training workflow was tracked using MLflow.</p> <p>This Application Package provides a CWL document that performs inference by applying the trained model to unseen Sentinel-2 data in order to generate a classified image. The CWL document contains a single main workflow that executes one <code>CommandLineTool</code> step. It also supports parallel execution by accepting a list of Sentinel-2 references as input, making it suitable for running at scale on a Minikube cluster.</p> <p>To execute the application, users have the option to use either cwltool or Calrissian as the CWL runner.</p>"},{"location":"inference-cwl/#inputs","title":"Inputs:","text":"<ul> <li><code>input_reference</code>: A list of Sentinel-2 product references from Planetary Computer. Note: the inference application provides accurate results only when the Sentinel-2 product has low or no cloud cover. High cloud coverage may significantly reduce prediction accuracy.</li> </ul>"},{"location":"inference-cwl/#how-to-execute-the-application-package","title":"How to Execute the Application Package","text":"<p>Before running the application with a CWL runner, make sure to download and use the latest version of the CWL document:</p> <pre><code>cd inference/app-package\nVERSION=$(curl -s https://api.github.com/repos/eoap/machine-learning-process/releases/latest | jq -r '.tag_name')\ncurl -L -o \"tile-sat-inference.cwl\" \\\n  \"https://github.com/eoap/machine-learning-process/releases/download/${VERSION}/tile-sat-inference.${VERSION}.cwl\"\n</code></pre>"},{"location":"inference-cwl/#run-the-application-package","title":"Run the Application Package:","text":"<p>There are two methods to execute the application:</p> <ul> <li> <p>Executing <code>tile-sat-inference</code> using <code>cwltool</code>:</p> <pre><code>cwltool --podman --debug --parallel tile-sat-inference.cwl#tile-sat-inference params.yml\n</code></pre> </li> <li> <p>Executing <code>tile-sat-inference</code> using <code>calrissian</code>:</p> <pre><code>calrissian --debug --stdout /calrissian/out.json --stderr /calrissian/stderr.log --usage-report /calrissian/report.json --max-ram 10G --max-cores 2 --parallel --tmp-outdir-prefix /calrissian/tmp/ --outdir /calrissian/results/ --tool-logs-basepath /calrissian/logs tile-sat-inference.cwl#tile-sat-inference params.yml\n</code></pre> <p>You can monitor the pod creation using command below:</p> <p><code>kubectl get pods</code> </p> </li> </ul>"},{"location":"inference-cwl/#how-the-cwl-document-is-designed","title":"How the CWL document is designed:","text":"<p>The CWL file can be triggered using <code>cwltool</code> or <code>calrissian</code>. The execution requires a <code>params.yml</code> file, which supplies all the necessary inputs defined in the CWL specification. The workflow is structured to run the module according to the diagram outlined below:</p> <p></p> <p>The Application Package will generate a number of directories containing intermediate and final outputs. Each directory will contain a <code>{STAC_ITEM_ID}_classified.tif</code> file, along with the corresponding STAC objects (i.e. the STAC Catalog and STAC Item). The number of directories depends on the number of input Sentinel-2 products provided. </p>"},{"location":"inference-cwl/#troubleshooting","title":"Troubleshooting","text":"<p>Users might encounter memory-related issues when executing workflows with CWL Runners (especially with <code>cwltool</code>). These issues can often be mitigated by reducing the <code>ramMax</code> parameter (e.g. <code>ramMax: 1000</code>) specified in the CWL file, which can help prevent excessive memory allocation.</p>"},{"location":"insights/","title":"Lessons Learned from Building a Machine Learning process for Geospatial Data","text":""},{"location":"insights/#introduction","title":"Introduction","text":"<p>This page highlights the technical challenges, design decisions, and key insights gained while developing the machine learning process for geospatial data pipeline. </p> <p>It also includes recommendations for future improvements and practical advice for replicating or extending the setup.</p>"},{"location":"insights/#design-decisions","title":"Design Decisions","text":""},{"location":"insights/#modular-workflow-templates","title":"Modular Workflow Templates","text":"<ul> <li> <p>Decision: Separate the CWL execution, training pipeline, and inference pipeline  into distinct workflow templates.</p> </li> <li> <p>Outcome: Enhanced reusability for other geospatial pipelines requiring similar preprocessing steps.</p> </li> </ul>"},{"location":"insights/#stac-integration","title":"STAC Integration","text":"<ul> <li>Decision: Leverage the STAC API, Geoparquet, and DuckDB for querying and storing geospatial data.</li> <li>Outcome: Improved interoperability with other geospatial tools and standards.</li> </ul>"},{"location":"insights/#tracking-the-process","title":"Tracking the process","text":"<ul> <li>Decision: Use MLFLOW exclusively for tracking the process of training workflow and selecting the best model candidate.</li> </ul>"},{"location":"insights/#test-inference-with-sentinel-2-product","title":"Test inference with Sentinel-2 product","text":"<ul> <li>Decision: Use Stars tool to stage-in a sentinel-2 product ready to pass to inference module.</li> </ul>"},{"location":"insights/#challenges-and-solutions","title":"Challenges and Solutions","text":""},{"location":"insights/#build-docker-images","title":"Build Docker Images","text":"<ul> <li>Challenge: Initially, we used an advanced tooling technique that leveraged Taskfile to build a Kaniko-based image and reference the CWL files. The image was then pushed to ttl.sh, a temporary image registry. This helps to execute the application packages using <code>calrissian</code>. However, this process was slow and hard to debug, often failing due to the large size of the Kaniko images.</li> <li>Solution: We now push the Docker images to a dedicated GitHub Container Registry.</li> </ul>"},{"location":"mlm/","title":"Describes a trained Machine Learning model","text":"<p>This tutorial describes a trained Machine Learning model using MLM STAC extension. The STAC MLM Extension provides a standard set of fields to describe machine learning models trained on overhead imagery and enable running model inference.</p> <p>The main objectives of the extension are:</p> <ul> <li>to enable building model collections that can be searched alongside associated STAC datasets;</li> <li>to record all necessary bands, parameters, modeling artifact locations, and high-level processing steps to deploy an inference service.</li> </ul> <p>For additional information please follow this Describe-MLmodel notebook.</p>"},{"location":"mlm/#for-developers","title":"For developers:","text":"<p>To run the notebook successfully, you must install the dependencies with <code>hatch</code>:</p> <pre><code>hatch shell prod\nhatch -e prod run python -m ipykernel install --user --name=mlm --display-name \"mlm\"\n</code></pre>"},{"location":"packages/","title":"Application Packages","text":"<p>This tutorial provides two separate application packages:</p> <ul> <li><code>training</code> </li> <li><code>inference</code></li> </ul> <p>Each application package has its own Docker image, which has been published to a dedicated GitHub Container Registry.</p> <p>For more details on how each package works, refer to the Reference Guides for training and inference.</p>"},{"location":"training-container/","title":"Training a Machine Learning Model- Container","text":"<p>This tutorial contains a Python application for training a deep learning model on EuroSAT dataset for tile-based classification task, and employs MLflow for monitoring the ML model development cycle. MLflow is a crucial tool that ensures effective log tracking and preserves key information, including specific code versions, datasets used, and model hyperparameters. By logging this information, the reproducibility of the work drastically increases, enabling users to revisit and replicate past experiments accurately. Moreover, quality metrics such as classification accuracy, loss function fluctuations, and inference time are also tracked, enabling easy comparison between different models. </p> <p>The EuroSAT dataset used in this tutorial consists of Sentinel-2 satellite images labeled with corresponding land use and cover categories. It provides a comprehensive representation of various land features. The dataset comprises 27,000 labeled and geo-referenced images, divided into 10 distinct classes. The multi-spectral version of the dataset includes all 13 Sentinel-2 bands, which retains the original value range of the Sentinel-2 bands, enabling access to a more comprehensive set of spectral information. This dataset has been published on the dedicated STAC endpoint. </p> <p></p>"},{"location":"training-container/#inputs","title":"Inputs","text":"<p>This application supports training the Convolutional Neural Network (CNN) model using either CPU or GPU to accelerate the process. It accepts the following input parameters:</p> Parameter Type Default Value Description stac_reference <code>str</code> <code>https://raw.githubusercontent.com/eoap/machine-learning-process/main/training/app-package/EUROSAT-Training-Dataset/catalog.json</code> URL pointing to a STAC catalog. The model reads GeoParquet annotations from the collection's assets. BATCH_SIZE <code>int</code> <code>2</code> Number of batches CLASSES <code>int</code> <code>10</code> Number of land cover classes to classify. DECAY <code>float</code> <code>0.1</code> Decay value used in training. EPOCHS<code>|</code>int` required Number of epochs EPSILON<code>|</code>float<code>|</code>1e-6` epsilon value (Model's heyperparameter) LEARNING_RATE<code>|</code>float<code>|</code>0.0001` Initial learning rate for the optimizer. LOSS <code>str</code> <code>categorical_crossentropy</code> Loss function for training. Options: <code>binary_crossentropy</code>, <code>cosine_similarity</code>, <code>mean_absolute_error</code>, <code>mean_squared_logarithmic_error</code>, <code>squared_hinge</code>. MEMENTUM <code>float</code> <code>0.95</code> Momentum parameter used in optimizers OPTIMIZER <code>str</code> <code>Adam</code> Optimization algorithm. Options: <code>Adam</code>, <code>SGD</code>, <code>RMSprop</code>. REGULARIZER <code>str</code> <code>None</code> Regularization technique to avoid overfitting. Options: <code>l1l2</code>,<code>l1</code>, <code>l2</code>, <code>None</code>. SAMPLES_PER_CLASS <code>int</code> <code>10</code> Number of samples to use for training per class."},{"location":"training-container/#outputs","title":"Outputs:","text":"<ul> <li><code>mlruns</code>: Directory containing artifacts, metrics, and metadata for each training run, tracked and organized by MLflow.</li> </ul>"},{"location":"training-container/#how-the-training-process-is-structured-internally","title":"How the training process is structured internally","text":"<p>The training pipeline for developing the training module encompasses four main components including:</p> <ul> <li> <p>Data Ingestion</p> </li> <li> <p>Based Model Architecture</p> </li> <li> <p>Training</p> </li> <li> <p>Evaluation</p> </li> </ul> <p>The pipeline for this task is illustrated in the diagram below:</p> <p></p>"},{"location":"training-container/#data-ingestion","title":"Data Ingestion","text":"<p>This component is designed to fetch data from a dedicated STAC endpoint containing a collection of STAC Items representing EuroSAT image chips. The user can query the collection using DuckDB on a GeoParquet file and split the resulting data into training, validation, and test datasets.</p>"},{"location":"training-container/#based-model-architecture","title":"Based Model Architecture","text":"<p>In this component, the user will design a CNN composed of seven distinct layers. The model beginnings with an input layer, which accepts images of shape <code>(13, 64, 64)</code> or any other cubic image shapes <code>(e.g. (3, 64, 64))</code>. This is followed by four convolutional blocks, each of which consists of the following sequence:</p> <ul> <li>A Conv2D layer with ReLU activation</li> <li>A BatchNormalization layer</li> <li>A 2D MaxPooling layer</li> <li>A Dropout layer</li> </ul> <p>Although each block contains multiple operations, they are collectively treated as four convolutional layers in the context of the model architecture.</p> <p>After these convolutional blocks, the model includes a dense (fully connected) layer, typically used to transition from the convolutional feature extraction to classification. Finally, the output layer is a Dense layer with 10 units (corresponding to the number of output classes) and uses the softmax activation function to produce a probability distribution over the classes. </p> <p>The user is required to select an appropriate loss function and optimizer from the available options. Once configured, the model is compiled and saved under <code>output/prepare_based_model</code>.</p>"},{"location":"training-container/#training","title":"Training","text":"<p>This component is responsible for training the model for a specified number of epochs, as provided by the user through the application package inputs.</p>"},{"location":"training-container/#evaluation","title":"Evaluation","text":"<p>The user can evaluate the trained model, and MLflow will track the process for each run under a designated experiment. Once the MLflow service is deployed and running on port <code>5000</code>, the UI can be accessed at http://localhost:5000.</p> <p>MLflow tracks the following:</p> <ul> <li>Evaluation metrics, including <code>Accuracy</code>, <code>Precision</code>, <code>Recall</code>, and the loss value  </li> <li>Trained machine learning model saved after each run  </li> <li>Additional artifacts, such as the Loss curve plot during training, and the Confusion matrix.</li> </ul>"},{"location":"training-container/#for-developers","title":"For developers","text":"<p>The folder structure is defined as: <code>src</code>/ <code>tile_based_training</code> /     - components /         - Containing all components such as <code>data_ingestion.py</code>, <code>prepare_base_model.py</code>, <code>train_model.py</code> , <code>model_evaluation.py</code>, <code>inference.py</code>.     - config /         - Containing all configuration needed for each component.     - utils /         - to define helper functions.     - pipeline /         - to define the order of executing for each component.</p>"},{"location":"training-cwl/","title":"Training Module &amp; CWL Runner","text":"<p>This Application Package provides a CWL document containing a top-level workflow with a single <code>CommandLineTool</code> step that executes the training pipeline. It also supports parallel execution, allowing users to specify multiple sets of hyperparameter or training configurations. This makes it suitable for large-scale experiments and hyperparameter tuning on platforms like a Minikube cluster.</p> <p>To execute the training workflow, users can choose between cwltool and Calrissian as their CWL runners.</p>"},{"location":"training-cwl/#inputs","title":"Inputs:","text":"Parameter Type Description MLFLOW_TRACKING_URI string An environment variable for the MLFLOW_TRACKING_URI stac_reference string URL pointing to a STAC catalog. The model reads GeoParquet annotations from the collection's assets. BATCH_SIZE int[] Number of batches CLASSES int Number of land cover classes to classify. DECAY float[] Decay value used in training. EPOCHS int[] Number of epochs EPSILON float[] Epsilon value (model hyperparameter) LEARNING_RATE float[] Initial learning rate for the optimizer. LOSS string[] Loss function for training. Options: <code>binary_crossentropy</code>, <code>cosine_similarity</code>, <code>mean_absolute_error</code>, <code>mean_squared_logarithmic_error</code>, <code>squared_hinge</code>. MEMENTUM float[] Momentum parameter used in optimizers OPTIMIZER string[] Optimization algorithm. Options: <code>Adam</code>, <code>SGD</code>, <code>RMSprop</code>. REGULARIZER string[] Regularization technique to avoid overfitting. Options: <code>l1l2</code>, <code>l1</code>, <code>l2</code>, <code>None</code>. SAMPLES_PER_CLASS int Number of samples to use for training per class."},{"location":"training-cwl/#how-to-execute-the-application-ppackage","title":"How to execute the Application Ppackage?","text":"<p>Before running the application with a CWL runner, make sure to download and use the latest version of the CWL document: <pre><code>cd training/app-package\nVERSION=$(curl -s https://api.github.com/repos/eoap/machine-learning-process/releases/latest | jq -r '.tag_name')\ncurl -L -o \"tile-sat-training.cwl\" \\\n  \"https://github.com/eoap/machine-learning-process/releases/download/${VERSION}/tile-sat-training.${VERSION}.cwl\"\n</code></pre></p>"},{"location":"training-cwl/#run-the-application-package","title":"Run the Application Package:","text":"<p>There are two methods to execute the application:</p> <ul> <li> <p>Executing the tile-based-training using <code>cwltool</code> in a terminal:    <pre><code> cwltool --podman --debug --parallel tile-sat-training.cwl#tile-sat-training params.yaml\n</code></pre></p> </li> <li> <p>Executing the tile-based classification using <code>calrissian</code> in a terminal:</p> </li> </ul> <pre><code> calrissian --debug --stdout /calrissian/out.json --stderr /calrissian/stderr.log --usage-report /calrissian/report.json --parallel --max-ram 10G --max-cores 2 --tmp-outdir-prefix /calrissian/tmp/ --outdir /calrissian/results/ --tool-logs-basepath /calrissian/logs tile-sat-training.cwl#tile-sat-training params.yaml\n</code></pre> <p>You can monitor the pod creation using <code>kubectl</code> command below:</p> <p><code>kubectl get pods</code> </p>"},{"location":"training-cwl/#how-the-cwl-document-is-designed","title":"How the CWL document is designed:","text":"<p>The CWL workflow can be executed using either <code>cwltool</code> or <code>calrissian</code>. The execution requires a <code>params.yml</code> file, which supplies all the necessary inputs defined in the CWL specification. The workflow is structured to run the module according to the diagram outlined below:</p> <p></p> <p><code>[]</code> in the image above indicates that the user may pass a list of parameters to the application package.</p>"},{"location":"training-cwl/#for-developers","title":"For developers","text":"<p>Users can train multiple tile-based classifiers using the CWL runner, with model weights tracked as artifacts in MLflow. Once training is complete, the next step is to retrieve the best-performing model, based on the chosen evaluation metric, from the MLflow artifact registry and convert it to ONNX format. This process is detailed in the \"Export the Best Model to ONNX Format\" guide. The resulting ONNX model can then be integrated into the inference application package.</p>"},{"location":"training-cwl/#troubleshooting","title":"Troubleshooting","text":"<p>Users might encounter memory-related issues when executing workflows with CWL Runners (especially with <code>cwltool</code>). These issues can often be mitigated by reducing the <code>ramMax</code> parameter (e.g. <code>ramMax: 1000</code>) specified in the CWL file, which can help prevent excessive memory allocation.</p>"}]}